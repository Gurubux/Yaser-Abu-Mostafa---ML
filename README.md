# Yaser-Abu-Mostafa---ML
<h2><a href='https://www.youtube.com/playlist?list=PLD63A284B7615313A'>Yaser Abu-Mostafa - ML - CS 156 - Caltech</a></h2>
<a href='http://work.caltech.edu/lectures.html'>Caltech Website - Homework - Slides</a><br>
<a href='http://work.caltech.edu/library/'>Index of Topics</a><br>
1. The Learning Problem - Introduction; supervised, unsupervised, and reinforcement learning. Components of the learning problem.<br>
2. Is Learning Feasible? - Can we generalize from a limited sample to the entire space? Relationship between in-sample and out-of-sample.<br>
3. The Linear Model I - Linear classification and linear regression. Extending linear models through nonlinear transforms.<br>
4. Error and Noise - The principled choice of error measures. What happens when the target we want to learn is noisy.<br>
5. Training versus Testing - The difference between training and testing in mathematical terms. What makes a learning model able to generalize?<br>
6. Theory of Generalization - How an infinite model can learn from a finite sample. The most important theoretical result in machine learning.<br>
7. The VC Dimension - A measure of what it takes a model to learn. Relationship to the number of parameters and degrees of freedom.n<br>
8. Bias-Variance Tradeoff - Breaking down the learning performance into competing quantities. The learning curves.<br>
9. The Linear Model II - More about linear models. Logistic regression, maximum likelihood, and gradient descent.<br>
10. Neural Networks - A biologically inspired model. The efficient backpropagation learning algorithm. Hidden layers.<br>
11. Overfitting - Fitting the data too well; fitting the noise. Deterministic noise versus stochastic noise.<br>
12. Regularization - Putting the brakes on fitting the noise. Hard and soft constraints. Augmented error and weight decay.<br>
13. Validation - Taking a peek out of sample. Model selection and data contamination. Cross validation.<br>
14. Support Vector Machines(SVM) -  One of the most successful learning algorithms; getting a complex model at the price of a simple one.<br>
15. Kernel Methods - Extending SVM to infinite-dimensional spaces using the kernel trick, and to non-separable data using soft margins.<br>
16. Radial Basis Functions - An important learning model that connects several machine learning models and techniques.<br>
17. Three Learning Principles - Major pitfalls for machine learning practitioners; Occam's razor, sampling bias, and data snooping.<br>
18. Epilogue - The map of machine learning. Brief views of Bayesian learning and aggregation methods.<br>
